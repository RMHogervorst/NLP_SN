---
title: "word2vec like julia"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

I wanted to use Julia Silge her work to apply word 2 vec
on the huge podcast archive.

https://juliasilge.com/blog/word-vectors-take-two/

take a sample of the entire scraped/downloaded archive.

```{r}
library(tidytext)
library(tidyverse)
library(widyr) # you need the newest version 1.2
```

The datafile is a row per episode. 

```{r take sample}
sn_complete <- read_rds("df_sn.RDS")
dim(sn_complete)
format(object.size(sn_complete),units = "Mb")
set.seed(13455)
sn_sample <- sample_n(sn_complete, 5)
rm(sn_complete) # make some space
```

```{r}
slide_windows <- function(tbl, doc_var, window_size) {
    # each word gets a skipgram (window_size words) starting on the first
    # e.g. skipgram 1 starts on word 1, skipgram 2 starts on word 2
    
    each_total <- tbl %>% 
        group_by(!!doc_var) %>% 
        mutate(doc_total = n(),
               each_total = pmin(doc_total, window_size, na.rm = TRUE)) %>%
        pull(each_total)
    
    rle_each <- rle(each_total)
    counts <- rle_each[["lengths"]]
    counts[rle_each$values != window_size] <- 1
    
    # each word get a skipgram window, starting on the first
    # account for documents shorter than window
    id_counts <- rep(rle_each$values, counts)
    window_id <- rep(seq_along(id_counts), id_counts)

    
    # within each skipgram, there are window_size many offsets
    indexer <- (seq_along(rle_each[["values"]]) - 1) %>%
        map2(rle_each[["values"]] - 1,
             ~ seq.int(.x, .x + .y)) %>% 
        map2(counts, ~ rep(.x, .y)) %>%
        flatten_int() +
        window_id
    
    tbl[indexer, ] %>%
        bind_cols(data_frame(window_id)) %>%
        group_by(window_id) %>%
        filter(n_distinct(!!doc_var) == 1) %>%
        ungroup
}
```

First have to unpack the nested dataframe. 

```{r}

tidy_pmi <- sn_sample %>%
    unnest(text) %>% 
    unnest_tokens(word, text) %>%
    add_count(word) %>%
    filter(n >= 20) %>%
    select(-n) %>%
    slide_windows(quo(ep_nr), 8) %>%
    pairwise_pmi(word, window_id)

```


```{r}
tidy_word_vectors <- tidy_pmi %>%
    widely_svd(item1, item2, pmi, nv = 256, maxit = 1000)
```

find nearest words

```{r}
nearest_synonyms <- function(df, token) {
    df %>%
        widely(~ . %*% (.[token, ]), sort = TRUE)(item1, dimension, value) %>%
        select(-item2)
}

tidy_word_vectors %>%
    nearest_synonyms("microsoft")
# in sample not: breach python, virus
tidy_word_vectors %>%
    nearest_synonyms("apple")
tidy_word_vectors %>%
    nearest_synonyms("google")
tidy_word_vectors %>%
    nearest_synonyms("computer")  # not: bitcoin, crypto, crypocurrency
tidy_word_vectors %>%
    nearest_synonyms("safe")
tidy_word_vectors %>%
    nearest_synonyms("program") # broken, fail, owned, open,program
```




What are some analogies we can find in this Hacker News corpus? Letâ€™s write a little function that will find the answer to token1 - token2 + token 3 = ???.
```{r}
analogy <- function(df, token1, token2, token3) {
    df %>%
        widely(~ . %*% (.[token1, ] - .[token2, ] + .[token3, ]), sort = TRUE)(item1, dimension, value) %>%
        select(-item2)
    
}

## operating systems
tidy_word_vectors %>%
    analogy("osx", "apple", "microsoft")
```

