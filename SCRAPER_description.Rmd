---
title: "Security now! scraper"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## First things first:

I will read in all of the SN transcripts, these are .txt files so in a way
we are not even scraping but just reading in 700 or so text files which is 
not really a big deal. 
However: In general it is nice if you ask permission (I did) and don't 
push the website to its limit. the GRC servers are quite beefy and I will 
probably not even make a dent in them, but be nice. (see also my blogpost about this: [scraping a website](http://rmhogervorst.nl/cleancode/blog/2017/12/08/downloading-multiple-files-and-failing.html))

There are multiple ways I could do the scraping:
if I had used [rvest](https://cran.r-project.org/web/packages/rvest/index.html) to scrape a website I would have set a user-agent
header^[a piece of information we send with every request that describes who we are and where to contact me if things go wrong]
and I would have used incremental backoff; when the server refuses a connection
we would wait and retry again, if it still refuses we would wait twice as long
and retry again etc.

## approach one: download directly with read_lines
However in this project we can just use read_lines^[This is the readr variant of readLines from base-R, it is much faster then the original (twice as fast?)]
to read the txt file of a transcript and apply further work downstream. 

**Alright, that did not work as planned, and was not fault tolerant**

The txt file is very nicely structured, that means we can extract pieces 
that contain the metadata. *See the scraper.R file in the folder R*

I have now downloaded everything into memory. which is not realy failt tolerant.
it also takes very long, and since I dont' know how long it takes and if the scraper
stops somewhere, I have added a helperfunction that prints  to the 
screen^[This is not really a pure function anymore, but it has side effects (it both reads and print where it is) but I needed to know where it failed]

## Approach 2
It did indeed fail in between and so I had to redownload files.
see [R/scraper.R] for more details.

I seperated the concerns: download, check, reading in the file to dataframe.

I've made a scraper that first generated all the links, than walks over those
links to download the files to disk. If a file is already downloaded, we skip
that one. In the second step I threw out the files that were missing (pages that do not exist will return something (a 404)).


**Now that I've downloaded all the transcripts I have to extract features from the files** 
You can find more about the extraction in [extracting features](extracting_features.md)


### about this document:

If you think this looks polished and that I am the wizard of R, think again,
and read through the git commits on github. You will see that I make stupid 
mistakes and correct them later on (or not). So learn from your missteps and put them
online, I encourage you and hope to see you soon. 


